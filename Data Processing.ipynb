{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3e9e44",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import nltk, random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "import pydot\n",
    "import pydotplus\n",
    "from pydotplus import graphviz\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544970e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']= str(seed_value)\n",
    "\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "rn.seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918e43e",
   "metadata": {},
   "source": [
    "# Creating a BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.vocab) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7b48e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################Importing Train Dataset##########################\n",
    "\n",
    "train = pd.read_csv(\"Preprocess_Train.csv\", header=0)\n",
    "train = train[train['text'].notnull()]\n",
    "train = train[train['target'].notnull()]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c097f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Split Train Dataset to Xtrain & Ytrain##########################\n",
    "\n",
    "Xtrain = train['text']\n",
    "Ytrain = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b727d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Tokenizing & Padding of Train Dataset##########################\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "Xtrain = '[CLS]' + Xtrain + '[SEP]'\n",
    "Xtrain = list(map(tokenizer.tokenize, Xtrain))\n",
    "\n",
    "c = Xtrain\n",
    "\n",
    "Xtrain = [tokenizer.convert_tokens_to_ids(txt) for txt in Xtrain]\n",
    "Xtrain = pad_sequences(Xtrain, padding='post', maxlen=maxlen)\n",
    "Xtrain = np.array(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afe013",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Importing Test Dataset##########################\n",
    "\n",
    "test = pd.read_csv(\"Preprocess_Test.csv\", header=0)\n",
    "test = test[test['text'].notnull()]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = test['text']\n",
    "Ytest = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Tokenizing & Padding of Test Dataset##########################\n",
    "\n",
    "Xtest = '[CLS]' + Xtest + '[SEP]'\n",
    "Xtest = list(map(tokenizer.tokenize, Xtest))\n",
    "Xtest = [tokenizer.convert_tokens_to_ids(txt) for txt in Xtest]\n",
    "Xtest = pad_sequences(Xtest, padding='post', maxlen=maxlen)\n",
    "Xtest = np.array(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107bc0c",
   "metadata": {},
   "source": [
    "# Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f480ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Loading Pretrained Word Embeddings to Create Feature Matrix##########################\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('data_embedding/glove/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for index in range(len(c)):\n",
    "    for word in c[index]:\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486dde13",
   "metadata": {},
   "source": [
    "# Creating the Model Using Deep Learning Techniques for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs1 = []\n",
    "pooled_outputs2 = []\n",
    "pooled_outputs3 = []\n",
    "\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "embedding_dim = 100\n",
    "embed_input = Input(shape=(maxlen,))\n",
    "\n",
    "dropout=0.1\n",
    "filter_sizes = [3,4,5,6]\n",
    "num_filters = [60,80,100,200]\n",
    "\n",
    "###################Embedding Layer##########################\n",
    "\n",
    "x = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen)(embed_input)\n",
    "\n",
    "###################Attention Layer##########################\n",
    "\n",
    "for i in range(1):\n",
    "    att_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)\n",
    "    att_output = Dropout(dropout)(att_output, training=False)\n",
    "    layer1_output = LayerNormalization(epsilon=1e-6)(att_output + x)\n",
    "    ff1_output = Sequential([Dense(ff_dim, activation=\"relu\"),Dense(embedding_dim)])(layer1_output)\n",
    "    ff1_output = Dropout(dropout)(ff1_output, training=False)\n",
    "    t = LayerNormalization(epsilon=1e-6)(ff1_output + layer1_output)\n",
    "    \n",
    "    pooled_outputs2.append(t)\n",
    "\n",
    "merge2 = concatenate(pooled_outputs2)\n",
    "\n",
    "###################CNN Layers##########################\n",
    "for j in range(len(filter_sizes)):\n",
    "    for i in range(1):\n",
    "        conv = (Convolution1D(filters=num_filters[i],\n",
    "                              kernel_size=filter_sizes[i],\n",
    "                              padding=\"same\",\n",
    "                              activation=\"relu\"))(merge2)\n",
    "        conv = (Convolution1D(filters=num_filters[i+1],\n",
    "                              kernel_size=filter_sizes[i+1],\n",
    "                              padding=\"same\",\n",
    "                              activation=\"relu\"))(conv)\n",
    "        conv = (Convolution1D(filters=num_filters[i+2],\n",
    "                              kernel_size=filter_sizes[i+2],\n",
    "                              padding=\"same\",\n",
    "                              activation=\"relu\"))(conv)\n",
    "        conv = (Convolution1D(filters=num_filters[i+3],\n",
    "                              kernel_size=filter_sizes[i+3],\n",
    "                              padding=\"same\",\n",
    "                              activation=\"relu\"))(conv)\n",
    "        pooled_outputs1.append(conv)\n",
    "    \n",
    "merge1 = concatenate(pooled_outputs1)\n",
    "\n",
    "###################LSTM Layers##########################\n",
    "\n",
    "# x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(merge1)\n",
    "\n",
    "x = Flatten()(merge1)\n",
    "x = Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[embed_input] , outputs=[x])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "import pydotplus\n",
    "import tensorflow as tf\n",
    "from pydotplus import graphviz\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Compiling our Model##########################\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=8e-4)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a466d2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################Training our Model##########################\n",
    "\n",
    "plrty = ['0', '1']\n",
    "result = []\n",
    "preds = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(i, ':')\n",
    "    history = model.fit(Xtrain, Ytrain, validation_split=0.2, epochs=1, batch_size=512)\n",
    "    preds.append(np.round(model.predict(Xtest)))\n",
    "#     print(classification_report(Ytest, preds, target_names=plrty))\n",
    "    results = f1_score(Ytest, preds[i], average='macro')\n",
    "    print(i, 'result :',results)\n",
    "    result.append(results)\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5be3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.savetxt(\"submission-1.csv\", preds, delimiter=\",\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99f766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
